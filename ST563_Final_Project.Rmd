---
  title: "Final Project: Analysis of Wine Quality Data"
author: "Dan Haines, Marcus Lee, Alex Prevatte"
date: "7/24/2021"
output: pdf_document
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Appendix

```{r}
# The wine data
# Method of analysis
# Any relevant comparison between the methods
# Findings
# Any issues that came up
```

```{r, message=FALSE, warning=FALSE}
# Reading in the libraries
library(tidyverse)
library(dplyr)
library(caret)
library(readr)
library(knitr)
library(regclass)
library(formatR)
library(e1071)
library(kernlab)
library(ROCR)
library(gridExtra)
```

```{r}
# Reading in the dataset
red <- read.csv("winequality-red.csv", sep=";")
str(red)
```

```{r}
# Setting seed for reproducibility 
set.seed(123)
# wine_data$quality <- as.factor(wine_data$quality)
# Dividing the data randomly into two sets
# A training set that I will use to fit the models
# A test set that will be used to evaluate the methods.
trainIndex <- createDataPartition(red$quality, p = 0.7,
                                  list = FALSE)
train.red <- red[trainIndex, ]
test.red <- red[-trainIndex, ]
```


## Multiple Linear Regression

```{r}
# select the model using best subset selection
regfit_best = regsubsets(quality~., data=train.red, nvmax=11)

# create a test matrix
test_mat = model.matrix(quality~., data=test.red)

# create a vector to contain all test MSE
val_errors = rep(NA,11)

# calculate the test MSE for the best model of each size
for(i in 1:11){
  coefi=coef(regfit_best, id=i)
  pred=test_mat[,names(coefi)]%*%coefi
  val_errors[i]=sqrt(mean((test.red$quality-pred)^2))
}

# plot the test MSE by variable number
plot(seq(1:11), val_errors, type='b',
     ylab='RMSE',
     xlab='Number of Variables',
     main='Test RMSE from Best Subset Selection')

# get the coefficient estimates for the 6-variable model
coef(regfit_best, 6)

# obtain the test RMSE for the final model
val_errors[6]
```

## Logistic Regression

```{r}
# Setting seed for reproducibility 
set.seed(123)
red2 <- red
red2$quality <- ifelse(red$quality >= 6, 1, 0)
red2$quality <- as.factor(red2$quality)

# Dividing the data randomly into two sets
# A training set that I will use to fit the models
# A test set that will be used to evaluate the methods.
trainIndex2 <- createDataPartition(red2$quality, p = 0.7,
                                   list = FALSE)
train.red2 <- red2[trainIndex2, ]
test.red2 <- red2[-trainIndex2, ]
```

```{r}
# Fitting a binary logistic regresison model
model <- glm(quality ~ alcohol + volatile.acidity + sulphates +
               I(volatile.acidity^2), data = train.red2, family = "binomial")
# Model Summary
summary(model)
# Response: This has a very low AIC
```


```{r}
# Logistic regression model fitting
logfit <- train(quality ~ alcohol + volatile.acidity + sulphates +
                  I(volatile.acidity^2), data = train.red2, method = "glm",
                family = "binomial", preProcess = c("center", "scale"), trControl = trainControl(method = "cv",
                                                                                                 number = 5))
logfit
```

```{r}
# Seeing how well Marcus model performs on the test set using accuracy for the logistic modeling
log_conf = confusionMatrix(data = test.red2$quality, reference = predict(logfit,
                           newdata = test.red2))

# produce a kable
kable(log_conf$table, caption="Confusion Matrix\nfor Logistic Regression ")
```

```{r}
misclass0 <- 1- ((160+180)/(160+63+76+180))
misclass0
```


# Classification tree

```{r}
trctrl <- trainControl(method= "repeatedcv", number = 10, repeats = 3)
# Create a classification tree
cTree <- train(quality ~ ., method = "rpart", trControl = trctrl, data = train.red2,
               preProcess = c("center", "scale"))
cTree

# plot the final tree model
plot(cTree$finalModel)
text(cTree$finalModel)
```

```{r}
# predict the values for our respone variable and compare it to our\ testing data
cTree_pred <- predict(cTree, newdata=select(test.red2,-quality))
# a frequency of how many of each response there is.
cTreepred <- table(cTree_pred, test.red2$quality)
#cTreepred
misclass1 <- 1- (sum(diag(cTreepred))/sum(cTreepred))
misclass1

# produce a kable
kable(cTreepred, caption="Confusion Matrix\nfor Classification Tree")
```


# Random Forest model

```{r}
# Create a random forest model
rforest <- train(quality ~ . , method = "rf", trControl = trctrl, data = train.red2, preProcess = c("center", "scale"))
rforest
```

```{r}
# Predict the values for our response variable and compare it to our testing data.
rforest_pred <- predict(rforest, newdata = select(test.red2,-quality))

# a frequency of how many of each respons]e there is.
rfpred <- table(rforest_pred, test.red2$quality)
misclass2 <- 1- (sum(diag(rfpred))/sum(rfpred))
misclass2

# produce a kable
kable(rfpred, caption="Confusion Matrix\nfor Random Forest")
```

# K-Nearest Neighbors
```{r}
### KNN - Classification

# set global training control options
train_control = trainControl(method='cv', number=5)

# fit the model
knn_class_fit <- train(quality ~ .,
                       method='knn',
                       tuneGrid=expand.grid(k=1:10),
                       trControl=train_control,
                       metric="Accuracy",
                       data=train.red2)

# predict on the KNN classification model
knn_conf = confusionMatrix(data=test.red2$quality, 
                           reference=predict(knn_class_fit,newdata=test.red2))

# produce a kable
kable(knn_conf$table, caption="Confusion Matrix\nfor KNN")

### KNN - Regression

# Calculate test MSE for K values
knn_RMSE = rep(NA,10)
for(i in 1:10){
  knn_reg_fit <- train(quality ~ .,
                       method='knn',
                       tuneGrid=expand.grid(k=i),
                       trControl=train_control,
                       metric="RMSE",
                       data=train.red)
  knn_preds = predict(knn_reg_fit, newdata=test.red)
  knn_RMSE[i] = RMSE(knn_preds, test.red$quality)
}

# plot the RMSE by K
plot(seq(1:10), knn_RMSE, type='b',
     xlab='Value of K',
     ylab='RMSE',
     main='Test RMSE by Value of K')
```

# Support Vector Machines
```{r}
### SVM - Linear Classification

# fit the model
svm_class_linear <- train(quality ~ .,
                          method='svmLinear',
                          preProcess=c('center','scale'),
                          trControl=train_control,
                          metric="Accuracy",
                          data=train.red2)

# predict on the SVM linear classification model
svm_lin_conf = confusionMatrix(data=test.red2$quality, 
                               reference=predict(svm_class_linear,newdata=test.red2))

# produce a kable
kable(svm_lin_conf$table, caption="Confusion Matrix\nfor SVM - Linear")

### SVM - Linear Regression
svm_reg_linear <- train(quality ~ .,
                        method='svmLinear',
                        preProcess=c('center','scale'),
                        trControl=train_control,
                        metric="RMSE",
                        data=train.red)

# predict on the svm linear regression model
svm_linear_preds <- predict(svm_reg_linear, newdata=test.red)
RMSE(svm_linear_preds, test.red$quality)

### SVM - Poly Classification

# fit the model
svm_class_poly <- train(quality ~ .,
                        method='svmPoly',
                        preProcess=c('center','scale'),
                        trControl=train_control,
                        metric="Accuracy",
                        data=train.red2)

# predict on the SVM poly classification model
svm_poly_conf = confusionMatrix(data=test.red2$quality, 
                                reference=predict(svm_class_poly,newdata=test.red2))

# produce a kable
kable(svm_poly_conf$table, caption="Confusion Matrix\nfor SVM - Polynomial")

### SVM - Poly Regression

# fit the model
svm_reg_poly <- train(quality ~ .,
                      method='svmPoly',
                      preProcess=c('center','scale'),
                      trControl=train_control,
                      metric="RMSE",
                      data=train.red)

# predict on the svm poly regression model
svm_poly_preds <- predict(svm_reg_poly, newdata=test.red)
RMSE(svm_poly_preds, test.red$quality)

### SVM - Radial Classification

# fit the model
svm_class_radial <- train(quality ~ .,
                          method='svmRadial',
                          preProcess=c('center','scale'),
                          trControl=train_control,
                          metric="Accuracy",
                          data=train.red2)

# predict on the SVM radial classification model
svm_radial_conf = confusionMatrix(data=test.red2$quality, 
                                  reference=predict(svm_class_radial,newdata=test.red2))

# produce a kable
kable(svm_radial_conf$table, caption="Confusion Matrix\nfor SVM - Radial")


### SVM - Radial Regression

# fit the model
svm_reg_radial <- train(quality ~ .,
                        method='svmRadial',
                        preProcess=c('center','scale'),
                        trControl=train_control,
                        metric="RMSE",
                        data=train.red)

# predict on the SVM radial regression model
svm_radial_preds <- predict(svm_reg_radial, newdata=test.red)
RMSE(svm_radial_preds, test.red$quality)
```

```{r}
# plot histograms for all variables
quality = ggplot(red, aes(x=red$quality)) + 
  geom_histogram(binwidth=1, fill='#CC0000', color='#000000') +
  xlab('Quality') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Quality')

fixed_acidity = ggplot(red, aes(x=red$fixed.acidity)) + 
  geom_histogram(binwidth=1, fill='#CC0000', color='#000000') +
  xlab('Fixed Acidity') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Fixed Acidity')

volatile_acidity = ggplot(red, aes(x=red$volatile.acidity)) + 
  geom_histogram(fill='#CC0000', color='#000000') +
  xlab('Volatile Acidity') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Volatile Acidity')

citric_acid = ggplot(red, aes(x=red$citric.acid)) + 
  geom_histogram(fill='#CC0000', color='#000000') +
  xlab('Citric Acid') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Citric Acid')

residual_sugar = ggplot(red, aes(x=red$residual.sugar)) + 
  geom_histogram(fill='#CC0000', color='#000000') +
  xlab('Residual Sugar') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Residual Sugar')

chorides = ggplot(red, aes(x=red$chlorides)) + 
  geom_histogram(fill='#CC0000', color='#000000') +
  xlab('Chlorides') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Chlorides')

free_sulfur = ggplot(red, aes(x=red$free.sulfur.dioxide)) + 
  geom_histogram(binwidth=1, fill='#CC0000', color='#000000') +
  xlab('Free Sulfur Dioxide') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Free Sulfur Dioxide')

total_sulfur = ggplot(red, aes(x=red$total.sulfur.dioxide)) + 
  geom_histogram(binwidth=10, fill='#CC0000', color='#000000') +
  xlab('Total Sulfur Dioxide') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Total Sulfur Dioxide')

density = ggplot(red, aes(x=red$density)) + 
  geom_histogram( fill='#CC0000', color='#000000') +
  xlab('Density') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Density')

ph = ggplot(red, aes(x=red$pH)) + 
  geom_histogram(binwidth=0.25, fill='#CC0000', color='#000000') +
  xlab('pH') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - pH')

sulphates = ggplot(red, aes(x=red$sulphates)) + 
  geom_histogram(binwidth=fill='#CC0000', color='#000000') +
  xlab('Sulphates') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Sulphates')

alcohol = ggplot(red, aes(x=red$alcohol)) + 
  geom_histogram(binwidth=0.5, fill='#CC0000', color='#000000') +
  xlab('Alcohol') +
  ylab('Count') + 
  ggtitle('Frequency Histogram - Alcohol')

grid.arrange(quality, fixed_acidity, volatile_acidity, citric_acid, residual_sugar, chorides)
grid.arrange(free_sulfur, total_sulfur, density, ph, sulphates, alcohol)

# create a correlation plot
cor_red = cor(red)
corrplot(cor_red, method='color', type='upper', addCoef.col='black', diag=FALSE)
```
