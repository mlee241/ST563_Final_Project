---
title: "Final Project: Analysis of Wine Quality Data"
author: "Dan Haine, Marcus Lee, Alex Prevatte"
date: "7/24/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Appendix

```{r}
# The wine data
# Method of analysis
# Any relevant comparison between the methods
# Findings
# Any issues that came up
```

```{r, message=FALSE, warning=FALSE}
# Reading in the libraries
library(tidyverse)
library(dplyr)
library(caret)
library(readr)
library(knitr)
library(regclass)
library(formatR)
library(e1071)
library(kernlab)
```

```{r}
# Reading in the dataset
red <- read.csv("winequality-red.csv", sep=";")
str(red)
```

```{r}
# Setting seed for reproducibility 
set.seed(123)
# wine_data$quality <- as.factor(wine_data$quality)

# Dividing the data randomly into two sets
# A training set that I will use to fit the models
# A test set that will be used to evaluate the methods.
trainIndex <- createDataPartition(red$quality, p = 0.7,
list = FALSE)
train.red <- red[trainIndex, ]
test.red <- red[-trainIndex, ]
```


## Multiple Linear Regression

```{r}
wine_quality_model = lm(quality ~ alcohol + volatile.acidity + sulphates,
data = train.red)
summary(wine_quality_model)
```

```{r}
anova(wine_quality_model)
```

```{r}
# This multiple linear regression model that Marcus ran had the smallest VIF
VIF(wine_quality_model)
```

```{r}
# Using cross validation to select my model
fit <- train(quality ~ alcohol + volatile.acidity + sulphates,
data = train.red, method = "lm", preProcess = c("center",
"scale"), trControl = trainControl(method = "cv", number = 5))
fit$results
```

```{r}
# Seeing how well Marcus model performs on the test set using squared error loss(RMSE) for MLR

# MLR
pred1 <- predict(fit, newdata=test.red)
postResample(pred1, obs = test.red$quality)
```

## Logistic Regression

```{r}
# Setting seed for reproducibility 
set.seed(123)

red2 <- red
red2$quality <- ifelse(red$quality >= 6, 1, 0)
red2$quality <- as.factor(red2$quality)

# Dividing the data randomly into two sets
# A training set that I will use to fit the models
# A test set that will be used to evaluate the methods.
trainIndex2 <- createDataPartition(red2$quality, p = 0.7,
list = FALSE)
train.red2 <- red2[trainIndex2, ]
test.red2 <- red2[-trainIndex2, ]
```

```{r}
# Fitting a binary logistic regresison model
model <- glm(quality ~ alcohol + volatile.acidity + sulphates +
I(volatile.acidity^2), data = train.red2, family = "binomial")

# Model Summary
summary(model)

# Response: This has a very low AIC
```


```{r}
# Logistic regression model fitting
logfit <- train(quality ~ alcohol + volatile.acidity + sulphates +
I(volatile.acidity^2), data = train.red2, method = "glm",
family = "binomial", preProcess = c("center", "scale"), trControl = trainControl(method = "cv",
number = 5))

logfit
```

```{r}
# Seeing how well Marcus model performs on the test set using accuracy for the logistic modeling

confusionMatrix(data = test.red2$quality, reference = predict(logfit,
newdata = test.red2))
```

```{r}
misclass0 <- 1- ((160+180)/(160+63+76+180))
misclass0
```


# Classification tree

```{r}
trctrl <- trainControl(method= "repeatedcv", number = 10, repeats = 3)
# Create a classification tree
cTree <- train(quality ~ ., method = "rpart", trControl = trctrl, data = train.red2,
               preProcess = c("center", "scale"))
cTree
```

```{r}
# predict the values for our respone variable and compare it to our\ testing data
cTree_pred <- predict(cTree, newdata=select(test.red2,-quality))
# a frequency of how many of each response there is.
cTreepred <- table(cTree_pred, test.red2$quality)
#cTreepred

misclass1 <- 1- (sum(diag(cTreepred))/sum(cTreepred))
misclass1
```


# Random Forest model

```{r}
# Create a random forest model
rforest <- train(quality ~ . , method = "rf", trControl = trctrl, data = train.red2, preProcess = c("center", "scale"))

rforest
```

```{r}
# Predict the values for our response variable and compare it to our testing data.
rforest_pred <- predict(rforest, newdata = select(test.red2,-quality))

# a frequency of how many of each respons]e there is.
rfpred <- table(rforest_pred, test.red2$quality)

misclass2 <- 1- (sum(diag(rfpred))/sum(rfpred))
misclass2
```

# K-Nearest Neighbors
```{r}
### KNN - Classification

# fit the model
knn_class_fit <- train(quality ~ .,
                       method='knn',
                       tuneGrid=expand.grid(k=1:10),
                       trControl=train_control,
                       metric="Accuracy",
                       data=train.red2)

# predict on the KNN classification model
confusionMatrix(data=test.red2$quality, 
                reference=predict(knn_class_fit,newdata=test.red2))

### KNN - Regression

# fit the model
knn_reg_fit <- train(quality ~ .,
                     method='knn',
                     tuneGrid=expand.grid(k=1:10),
                     trControl=train_control,
                     metric="RMSE",
                     data=train.red)

# predict on the KNN regression model
knn_preds <- predict(knn_reg_fit, newdata=test.red)
RMSE(knn_preds, test.red$quality)
```

# Support Vector Machines
```{r}
### SVM - Linear Classification

# fit the model
svm_class_linear <- train(quality ~ .,
                          method='svmLinear',
                          preProcess=c('center','scale'),
                          trControl=train_control,
                          metric="Accuracy",
                          data=train.red2)

# predict on the SVM linear classification model
confusionMatrix(data=test.red2$quality, 
                reference=predict(svm_class_linear,newdata=test.red2))

### SVM - Linear Regression

# fit the model
svm_reg_linear <- train(quality ~ .,
                        method='svmLinear',
                        preProcess=c('center','scale'),
                        trControl=train_control,
                        metric="RMSE",
                        data=train.red)

# predict on the SVM linear regression model
svm_linear_preds <- predict(svm_reg_linear, newdata=test.red)
RMSE(svm_linear_preds, test.red$quality)

### SVM - Poly Classification

# fit the model
svm_class_poly <- train(quality ~ .,
                        method='svmPoly',
                        preProcess=c('center','scale'),
                        trControl=train_control,
                        metric="Accuracy",
                        data=train.red2)

# predict on the SVM poly classification model
confusionMatrix(data=test.red2$quality, 
                reference=predict(svm_class_poly,newdata=test.red2))

### SVM - Poly Regression

# fit the model
svm_reg_poly <- train(quality ~ .,
                      method='svmPoly',
                      preProcess=c('center','scale'),
                      trControl=train_control,
                      metric="RMSE",
                      data=train.red)


# predict on the svm poly regression model
svm_poly_preds <- predict(svm_reg_poly, newdata=test.red)
RMSE(svm_poly_preds, test.red$quality)

### SVM - Radial Classification

# fit the model
svm_class_radial <- train(quality ~ .,
                          method='svmRadial',
                          preProcess=c('center','scale'),
                          trControl=train_control,
                          metric="Accuracy",
                          data=train.red2)

# predict on the SVM radial classification model
confusionMatrix(data=test.red2$quality, 
                reference=predict(svm_class_radial,newdata=test.red2))

### SVM - Radial Regression

# fit the model
svm_reg_radial <- train(quality ~ .,
                        method='svmRadial',
                        preProcess=c('center','scale'),
                        trControl=train_control,
                        metric="RMSE",
                        data=train.red)

# predict on the KNN regression model
svm_radial_preds <- predict(svm_reg_radial, newdata=test.red)
RMSE(svm_radial_preds, test.red$quality)
```





















